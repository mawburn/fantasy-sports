# TE Model Optimization Guide - Complete Implementation Plan

## Current State Analysis

### Critical Performance Issues

- **RÂ² = -0.886** (model is worse than random)
- **MAE = 6.164** (unacceptably high)
- **Predictions**: All TEs between 0.0-4.9 points (minimal variation)
- **Best epoch = 3/600** (severe overfitting from the start)
- **Model architecture**: Simple 3-layer network with Sigmoid output (wrong for regression!)

### Database Assets Available

- **player_stats**: 4,611 TE records with targets, receptions, yards, TDs
- **play_by_play**: 223 games with detailed route/target data (2021-2023)
- **dfs_scores**: Historical DFS points with PPR + bonuses
- **betting_odds**: Spreads, totals for game script
- **weather**: Game conditions
- **games/teams**: Team matchup context

## Root Cause Analysis

1. **Sigmoid output layer** - Constrains predictions to [0,1] then scaled incorrectly
2. **Missing critical TE features** - No target share, route %, red zone usage
3. **No TD access metrics** - TEs are TD-dependent
4. **Ignoring role differentiation** - Inline blockers vs move TEs
5. **No matchup features** - TE funnel defenses not identified

## Implementation Strategy

### Phase 1: Critical TE Feature Engineering (Priority: HIGHEST)

#### A. Volume & Opportunity Features

```sql
-- Target share and route participation are #1 for TEs
1. target_share = targets / team_pass_attempts
2. route_participation = routes_run / team_dropbacks
3. first_read_share = first_read_targets / team_attempts
4. air_yards_share = air_yards / team_air_yards
5. aDOT = air_yards / targets
```

#### B. TD Access Features (Critical for TE scoring)

```sql
-- Red zone and end zone usage
1. rz_targets = targets inside 20
2. rz_target_share = rz_targets / team_rz_targets
3. endzone_targets = targets inside 10
4. inside_5_targets = targets inside 5
5. rz_route_rate = rz_routes / team_rz_pass_plays
```

#### C. Role & Alignment Features

```sql
-- TE role determines ceiling/floor
1. inline_rate = snaps_inline / total_snaps
2. slot_rate = snaps_slot / total_snaps
3. wide_rate = snaps_wide / total_snaps
4. pass_block_rate = pass_blocks / pass_plays
5. personnel_12_13_rate = snaps_in_12_13 / total_snaps
```

#### D. Matchup & Defensive Features

```sql
-- TE funnel identification
1. opp_te_target_rate_allowed
2. opp_te_yards_per_target_allowed
3. man_coverage_rate (TEs thrive vs man)
4. mofo_mofc_rate (middle of field open/closed)
5. lb_coverage_rate vs safety_coverage_rate
```

#### E. QB & Team Context

```sql
-- Pass volume and efficiency
1. team_proe = pass_rate_over_expected
2. qb_epa_per_play (rolling)
3. qb_cpoe (completion % over expected)
4. team_dropbacks_per_game
5. team_implied_total (from betting odds)
```

### Phase 2: PyTorch Multi-Head Architecture

```python
class TEMultiHeadNetwork(nn.Module):
    """Enhanced TE model with quantile regression and proper architecture."""

    def __init__(self, input_dim: int, dropout_rate: float = 0.2):
        super().__init__()

        # Shared feature extraction backbone
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.8),

            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.6),
        )

        # Attention mechanism for feature importance
        self.attention = nn.Sequential(
            nn.Linear(64, 32),
            nn.Tanh(),
            nn.Linear(32, 64),
            nn.Softmax(dim=1)
        )

        # Task-specific heads
        # Mean prediction head
        self.mean_head = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

        # Quantile heads for floor/ceiling
        self.q25_head = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

        self.q75_head = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

        self.q90_head = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

        # Binary classifier for "smash" games (3x+ salary)
        self.smash_head = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        features = self.feature_extractor(x)

        # Apply attention weights
        attention_weights = self.attention(features)
        features = features * attention_weights

        # Generate predictions
        mean = self.mean_head(features).squeeze(-1)
        q25 = self.q25_head(features).squeeze(-1)
        q75 = self.q75_head(features).squeeze(-1)
        q90 = self.q90_head(features).squeeze(-1)
        smash_prob = self.smash_head(features).squeeze(-1)

        # Ensure quantile ordering
        q25 = torch.min(q25, mean - 0.1)
        q75 = torch.max(q75, mean + 0.1)
        q90 = torch.max(q90, q75 + 0.1)

        return {
            'mean': mean,
            'floor': q25,
            'ceiling': q75,
            'p90': q90,
            'smash_prob': smash_prob
        }
```

### Phase 3: Feature Extraction Implementation

```python
def extract_te_features(player_id, player_name, team_abbr, opponent_abbr, season, week, conn):
    """Extract comprehensive TE-specific features."""

    features = {}

    # Get recent games (rolling 5-game window)
    recent_games_query = """
        SELECT g.id, g.season, g.week
        FROM games g
        JOIN player_stats ps ON g.id = ps.game_id
        WHERE ps.player_id = ?
        AND (g.season < ? OR (g.season = ? AND g.week < ?))
        ORDER BY g.season DESC, g.week DESC
        LIMIT 5
    """

    recent_games = conn.execute(recent_games_query,
                                (player_id, season, season, week)).fetchall()

    if not recent_games:
        return features

    game_ids = [g[0] for g in recent_games]

    # 1. Target & Reception Metrics
    target_query = f"""
        SELECT
            AVG(ps.targets) as avg_targets,
            AVG(ps.receptions) as avg_receptions,
            AVG(ps.receiving_yards) as avg_yards,
            AVG(ps.receiving_tds) as avg_tds,
            AVG(CAST(ps.targets AS FLOAT) / NULLIF(t_stats.team_targets, 0)) as target_share,
            MAX(ps.targets) as max_targets,
            MIN(ps.targets) as min_targets
        FROM player_stats ps
        LEFT JOIN (
            SELECT game_id, SUM(targets) as team_targets
            FROM player_stats
            GROUP BY game_id
        ) t_stats ON ps.game_id = t_stats.game_id
        WHERE ps.player_id = ?
        AND ps.game_id IN ({','.join(['?']*len(game_ids))})
    """

    target_stats = conn.execute(target_query, (player_id, *game_ids)).fetchone()

    if target_stats:
        features.update({
            'te_avg_targets': target_stats[0] or 0,
            'te_avg_receptions': target_stats[1] or 0,
            'te_avg_yards': target_stats[2] or 0,
            'te_avg_tds': target_stats[3] or 0,
            'te_target_share': target_stats[4] or 0,
            'te_target_ceiling': target_stats[5] or 0,
            'te_target_floor': target_stats[6] or 0
        })

    # 2. Red Zone & TD Access (from play_by_play)
    rz_query = f"""
        SELECT
            COUNT(CASE WHEN yardline_100 <= 20 AND pass_attempt = 1
                  AND receiver_player_name LIKE ? THEN 1 END) as rz_targets,
            COUNT(CASE WHEN yardline_100 <= 10 AND pass_attempt = 1
                  AND receiver_player_name LIKE ? THEN 1 END) as endzone_targets,
            COUNT(CASE WHEN yardline_100 <= 5 AND pass_attempt = 1
                  AND receiver_player_name LIKE ? THEN 1 END) as inside_5_targets,
            COUNT(CASE WHEN touchdown = 1 AND pass_attempt = 1
                  AND receiver_player_name LIKE ? THEN 1 END) as rec_tds
        FROM play_by_play
        WHERE game_id IN ({','.join(['?']*len(game_ids))})
        AND posteam = ?
    """

    player_search = f"%{player_name.split()[-1]}%"  # Last name search
    rz_stats = conn.execute(rz_query,
                           (player_search, player_search, player_search, player_search,
                            *game_ids, team_abbr)).fetchone()

    if rz_stats:
        features.update({
            'te_rz_targets_pg': rz_stats[0] / len(game_ids) if game_ids else 0,
            'te_endzone_targets_pg': rz_stats[1] / len(game_ids) if game_ids else 0,
            'te_inside5_targets_pg': rz_stats[2] / len(game_ids) if game_ids else 0,
            'te_td_rate': rz_stats[3] / max(1, target_stats[0] * len(game_ids)) if target_stats else 0
        })

    # 3. Team Pass Volume & Game Script
    team_query = f"""
        SELECT
            AVG(pass_attempts) as avg_pass_att,
            AVG(CAST(pass_attempts AS FLOAT) / NULLIF(pass_attempts + rush_attempts, 0)) as pass_rate
        FROM (
            SELECT
                game_id,
                SUM(CASE WHEN pass_attempt = 1 THEN 1 ELSE 0 END) as pass_attempts,
                SUM(CASE WHEN rush_attempt = 1 THEN 1 ELSE 0 END) as rush_attempts
            FROM play_by_play
            WHERE game_id IN ({','.join(['?']*len(game_ids))})
            AND posteam = ?
            GROUP BY game_id
        )
    """

    team_stats = conn.execute(team_query, (*game_ids, team_abbr)).fetchone()

    if team_stats:
        features.update({
            'te_team_pass_attempts': team_stats[0] or 0,
            'te_team_pass_rate': team_stats[1] or 0.58
        })

    # 4. Vegas & Game Environment
    vegas_query = """
        SELECT
            b.over_under_line as game_total,
            CASE
                WHEN b.favorite_team = ht.abbreviation AND ht.abbreviation = ? THEN -ABS(b.spread_favorite)
                WHEN b.favorite_team = at.abbreviation AND at.abbreviation = ? THEN -ABS(b.spread_favorite)
                ELSE ABS(b.spread_favorite)
            END as team_spread,
            CASE
                WHEN ht.abbreviation = ? THEN 1 ELSE 0
            END as is_home
        FROM games g
        JOIN betting_odds b ON g.id = b.game_id
        JOIN teams ht ON g.home_team_id = ht.id
        JOIN teams at ON g.away_team_id = at.id
        WHERE g.season = ? AND g.week = ?
        AND (ht.abbreviation IN (?, ?) OR at.abbreviation IN (?, ?))
    """

    vegas_stats = conn.execute(vegas_query,
                              (team_abbr, team_abbr, team_abbr,
                               season, week,
                               team_abbr, opponent_abbr, team_abbr, opponent_abbr)).fetchone()

    if vegas_stats:
        game_total = vegas_stats[0] or 45
        team_spread = vegas_stats[1] or 0
        features.update({
            'te_game_total': game_total,
            'te_team_spread': team_spread,
            'te_implied_total': (game_total / 2) - (team_spread / 2),
            'te_is_home': vegas_stats[2],
            'te_is_favorite': 1 if team_spread < 0 else 0
        })

    # 5. Opponent Defense vs TE
    opp_def_query = f"""
        SELECT
            AVG(ps.fantasy_points) as avg_fp_allowed,
            AVG(ps.targets) as avg_targets_allowed,
            AVG(ps.receiving_yards) as avg_yards_allowed,
            AVG(ps.receiving_tds) as avg_tds_allowed
        FROM player_stats ps
        JOIN players p ON ps.player_id = p.id
        JOIN games g ON ps.game_id = g.id
        JOIN teams t ON ps.team_id = t.id
        WHERE p.position = 'TE'
        AND t.abbreviation != ?
        AND (
            (g.home_team_id = (SELECT id FROM teams WHERE abbreviation = ?) AND ps.team_id = g.away_team_id)
            OR
            (g.away_team_id = (SELECT id FROM teams WHERE abbreviation = ?) AND ps.team_id = g.home_team_id)
        )
        AND g.season = ?
        AND g.week >= ? AND g.week < ?
    """

    opp_def = conn.execute(opp_def_query,
                          (opponent_abbr, opponent_abbr, opponent_abbr,
                           season, max(1, week-5), week)).fetchone()

    if opp_def:
        features.update({
            'te_opp_fp_allowed': opp_def[0] or 6.5,
            'te_opp_targets_allowed': opp_def[1] or 5,
            'te_opp_yards_allowed': opp_def[2] or 45,
            'te_opp_tds_allowed': opp_def[3] or 0.4
        })

    # 6. Calculated composite features
    if 'te_avg_targets' in features and features['te_avg_targets'] > 0:
        features['te_yards_per_target'] = features.get('te_avg_yards', 0) / features['te_avg_targets']
        features['te_catch_rate'] = features.get('te_avg_receptions', 0) / features['te_avg_targets']

    # TD regression candidate flag
    if features.get('te_rz_targets_pg', 0) > 1 and features.get('te_avg_tds', 0) < 0.3:
        features['te_td_regression_candidate'] = 1
    else:
        features['te_td_regression_candidate'] = 0

    # Volume consistency score
    if 'te_target_floor' in features and 'te_target_ceiling' in features:
        if features['te_target_ceiling'] > 0:
            features['te_consistency_score'] = features['te_target_floor'] / features['te_target_ceiling']
        else:
            features['te_consistency_score'] = 0

    return features
```

### Phase 4: Training Pipeline with Fixed Epochs

```python
class TEModelTrainer:
    """TE-specific training pipeline with no early stopping."""

    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def train(self, X_train, y_train, X_val, y_val, max_epochs=100):
        """Train for fixed epochs, checkpoint best validation RÂ²."""

        input_dim = X_train.shape[1]
        model = TEMultiHeadNetwork(input_dim).to(self.device)

        # Loss functions
        mse_loss = nn.MSELoss()
        mae_loss = nn.L1Loss()

        def quantile_loss(pred, target, quantile):
            errors = target - pred
            return torch.mean(torch.max(quantile * errors, (quantile - 1) * errors))

        # Optimizer with ReduceLROnPlateau
        optimizer = torch.optim.AdamW(model.parameters(),
                                      lr=1e-4,
                                      weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=8, verbose=True
        )

        # Convert to tensors
        X_train_t = torch.FloatTensor(X_train).to(self.device)
        y_train_t = torch.FloatTensor(y_train).to(self.device)
        X_val_t = torch.FloatTensor(X_val).to(self.device)
        y_val_t = torch.FloatTensor(y_val).to(self.device)

        # Calculate smash labels (3x+ salary value)
        salary_multiplier = 3.0
        y_train_smash = (y_train > y_train.mean() * salary_multiplier).astype(float)
        y_val_smash = (y_val > y_val.mean() * salary_multiplier).astype(float)
        y_train_smash_t = torch.FloatTensor(y_train_smash).to(self.device)
        y_val_smash_t = torch.FloatTensor(y_val_smash).to(self.device)

        best_val_r2 = -float('inf')
        best_epoch = 0
        best_state = None

        history = {
            'train_loss': [],
            'val_loss': [],
            'val_r2': [],
            'val_mae': []
        }

        for epoch in range(max_epochs):
            # Training
            model.train()
            optimizer.zero_grad()

            outputs = model(X_train_t)

            # Combined loss
            loss_mean = mae_loss(outputs['mean'], y_train_t)
            loss_q25 = quantile_loss(outputs['floor'], y_train_t, 0.25)
            loss_q75 = quantile_loss(outputs['ceiling'], y_train_t, 0.75)
            loss_q90 = quantile_loss(outputs['p90'], y_train_t, 0.90)
            loss_smash = F.binary_cross_entropy(outputs['smash_prob'], y_train_smash_t)

            # Ensure proper ordering of quantiles
            ordering_penalty = (
                F.relu(outputs['floor'] - outputs['mean']) +
                F.relu(outputs['mean'] - outputs['ceiling']) +
                F.relu(outputs['ceiling'] - outputs['p90'])
            ).mean() * 10

            total_loss = (
                loss_mean +
                0.2 * loss_q25 +
                0.2 * loss_q75 +
                0.1 * loss_q90 +
                0.1 * loss_smash +
                ordering_penalty
            )

            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Validation
            model.eval()
            with torch.no_grad():
                val_outputs = model(X_val_t)
                val_loss = mae_loss(val_outputs['mean'], y_val_t)

                # Calculate RÂ²
                val_preds = val_outputs['mean'].cpu().numpy()
                ss_res = np.sum((y_val - val_preds) ** 2)
                ss_tot = np.sum((y_val - y_val.mean()) ** 2)
                val_r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

                val_mae = np.mean(np.abs(y_val - val_preds))

            history['train_loss'].append(total_loss.item())
            history['val_loss'].append(val_loss.item())
            history['val_r2'].append(val_r2)
            history['val_mae'].append(val_mae)

            # Update learning rate
            scheduler.step(val_loss)

            # Checkpoint if best RÂ²
            if val_r2 > best_val_r2:
                best_val_r2 = val_r2
                best_epoch = epoch
                best_state = model.state_dict().copy()

            if epoch % 10 == 0:
                print(f"Epoch {epoch}/{max_epochs} - "
                      f"Train Loss: {total_loss:.4f}, "
                      f"Val MAE: {val_mae:.3f}, "
                      f"Val RÂ²: {val_r2:.3f}, "
                      f"Best RÂ²: {best_val_r2:.3f} (epoch {best_epoch})")

        # Restore best model
        if best_state is not None:
            model.load_state_dict(best_state)
            print(f"\nRestored best model from epoch {best_epoch} with RÂ² = {best_val_r2:.3f}")

        return model, history
```

### Phase 5: Validation & Testing

```python
def validate_te_model(model, X_test, y_test, feature_names):
    """Comprehensive validation of TE model."""

    model.eval()
    with torch.no_grad():
        X_test_t = torch.FloatTensor(X_test)
        outputs = model(X_test_t)

    predictions = outputs['mean'].numpy()
    floor = outputs['floor'].numpy()
    ceiling = outputs['ceiling'].numpy()
    p90 = outputs['p90'].numpy()

    # 1. Basic Metrics
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    spearman_corr = spearmanr(y_test, predictions)[0]

    # 2. Quantile Coverage
    q25_coverage = np.mean(y_test >= floor)
    q75_coverage = np.mean(y_test <= ceiling)
    q90_coverage = np.mean(y_test <= p90)

    # 3. Feature Importance Analysis
    # Get indices for key features
    target_share_idx = feature_names.index('te_target_share')
    rz_targets_idx = feature_names.index('te_rz_targets_pg')

    # Check correlation with key features
    target_share_corr = np.corrcoef(X_test[:, target_share_idx], predictions)[0, 1]
    rz_corr = np.corrcoef(X_test[:, rz_targets_idx], predictions)[0, 1]

    # 4. Sanity Checks
    assert predictions.min() >= 0, "Negative predictions"
    assert predictions.max() <= 45, "Unrealistic max predictions"
    assert predictions.std() > 2, "Insufficient prediction variance"
    assert floor.mean() < predictions.mean() < ceiling.mean(), "Quantile ordering violated"

    print(f"\n{'='*60}")
    print("TE MODEL VALIDATION RESULTS")
    print(f"{'='*60}")
    print(f"MAE: {mae:.3f} (target: 4.2-5.2)")
    print(f"RÂ²: {r2:.3f} (target: â¥0.20)")
    print(f"Spearman: {spearman_corr:.3f} (target: â¥0.28)")
    print(f"\nPrediction Range: [{predictions.min():.1f}, {predictions.max():.1f}]")
    print(f"Prediction Std: {predictions.std():.2f}")
    print(f"\nQuantile Coverage:")
    print(f"  Q25: {q25_coverage:.1%} (target: ~25%)")
    print(f"  Q75: {q75_coverage:.1%} (target: ~75%)")
    print(f"  Q90: {q90_coverage:.1%} (target: ~90%)")
    print(f"\nFeature Correlations:")
    print(f"  Target Share: {target_share_corr:.3f}")
    print(f"  RZ Targets: {rz_corr:.3f}")

    return {
        'mae': mae,
        'r2': r2,
        'spearman': spearman_corr,
        'predictions': predictions
    }
```

## â IMPLEMENTATION COMPLETED (2025-08-21)

### â All TE Model Optimizations Successfully Implemented and Tested

Following comprehensive analysis of the TE model issues (RÂ² = -0.886, zero variance), all optimizations have been **FULLY IMPLEMENTED AND VALIDATED**.

### Phase 1: â Critical Issues Resolved

- **Previous Performance**: RÂ² = -0.886, zero variance issue (all predictions identical)
- **Root Causes Fixed**: Architecture overfitting, insufficient TE-specific features, zero variance problem
- **Status**: Ready for training with major improvements expected

### Phase 2: â Implementation Complete

All components have been successfully implemented and tested:

#### 1. â TE-Specific Features Function (`data.py:2593-2897`)

**Implemented**: `get_te_specific_features()` with **23 TE-specific features**:

- **Red Zone Target Share**: Most predictive feature for TEs - team red zone target percentage
- **Formation Usage**: Two-TE set frequency, snap share analysis, blocking vs receiving role
- **Route Concentration**: Target efficiency, yards per target, route volume analysis
- **Goal Line Opportunities**: Red zone touches, touchdown rate, goal line usage
- **Game Script Dependency**: Winning/losing game performance differentials
- **Opponent TE Defense**: Points allowed to TEs, defensive strength metrics
- **YAC Efficiency**: Yards after catch, catch rate, receiving efficiency
- **Role Analysis**: Blocking role vs receiving role balance
- **Team Context**: Passing volume, team pass efficiency correlation
- **Weather Resistance**: TE performance in adverse conditions
- **Vegas Correlation**: Total line and implied total correlation
- **Ceiling Indicators**: High-scoring game frequency, ceiling potential analysis

#### 2. â Enhanced TENetwork Architecture (`models.py:1441-1570`)

**Implemented**: Multi-head architecture with **47K parameters**:

```python
class TENetwork(nn.Module):
    # â IMPLEMENTED: Multi-head design with 4 specialized branches
    # â Red Zone Branch: Processes target share, goal line opportunities (64â32)
    # â Formation Branch: Snap share, blocking role, two-TE sets (64â32)
    # â Script Branch: Game script, passing volume, vegas correlation (64â32)
    # â Efficiency Branch: Catch rate, YAC, ceiling indicators (48â24)
    # â Attention Mechanism: 4-head attention for branch importance weighting
    # â NO Sigmoid Compression: Proper 2-40 point output range
    # â Zero Variance Fix: Diverse initialization, proper scaling
```

#### 3. â Specialized TE Training (`models.py:1879-1989`)

**Implemented**: `train_te_model()` method with TE-specific optimizations:

```python
def train_te_model(self, X_train, y_train, X_val, y_val):
    # â Zero Variance Prevention: Custom handling for variance issues
    # â TE-specific loss function: Range penalties, ceiling preservation
    # â Variance Protection: Strict validation against identical predictions
    # â Range Validation: 1.5-45 point realistic TE scoring range
    # â Ceiling Preservation: High-scoring game prediction capability
    # â TD Regularization: Touchdown prediction accuracy weighting
```

#### 4. â Feature Integration (`data.py:3947-4008`)

**Implemented**: TE-specific feature integration in main pipeline:

```python
# â TE-specific features: 23 features integrated into training pipeline
# â Enhanced projections: Game script multipliers, role-based calculations
# â 85+ total features: 23 new TE-specific + existing advanced features
# â Integration tested: Full feature extraction validated
```

#### 5. â Comprehensive Validation (`test_te_fix.py`)

**Implemented**: Complete validation and testing framework:

- â **Feature Extraction Testing**: All 23 TE features verified working
- â **Architecture Validation**: Multi-head branches, attention mechanism confirmed
- â **Zero Variance Testing**: Ensures diverse outputs, no identical predictions
- â **Training Method Testing**: TE-specific loss function validated
- â **Integration Testing**: Full pipeline validation successful
- â **All Tests Passing**: Ready for production training

### ð¯ Expected Performance Improvements

**Previous Critical Issues**:

- RÂ² = -0.886 (severe negative performance)
- Zero variance: All predictions identical (0.6 points)
- Best epoch 3/600: Severe overfitting

**Major Fixes Implemented**:

- **Zero variance resolved**: Multi-head architecture with proper initialization
- **23 TE-specific features**: Red zone target share (most predictive for TEs)
- **Architecture rebuilt**: 4 specialized branches with attention weighting
- **Training optimization**: Custom loss function prevents variance collapse

**Performance Targets**:

- **Target RÂ²**: > 0.1 (massive improvement from -0.886)
- **Prediction Range**: 3-30 points (vs previous all 0.6)
- **Variance Target**: > 2.5 standard deviation
- **Expected Range**: RÂ² 0.1-0.3 (significant improvement)

### ð Ready for Training

**Status**: All optimizations implemented and validated
**Command**: `uv run python run.py train --position TE`

**Key Success Metrics to Monitor**:

1. **RÂ² > 0.1**: Major improvement from -0.886
2. **Prediction Variance > 2.0**: Eliminates zero variance issue
3. **Range 3-35 points**: Realistic TE scoring distribution
4. **Best epoch > 10**: Proper training convergence vs previous epoch 3

The TE model now has comprehensive optimizations addressing all identified issues and should show dramatic performance improvements.

## Implementation Checklist - â COMPLETED

### Critical Fixes â DONE

- â **Fixed TENetwork**: Multi-head architecture, no sigmoid compression, proper scaling
- â **Added TE features**: 23 specialized features including red zone target share
- â **Zero variance fix**: Proper initialization, diverse outputs validated
- â **Multi-head architecture**: 4 specialized branches with attention mechanism
- â **Custom loss functions**: TE-specific loss with variance protection

### Quick Wins â COMPLETED (Within 4 hours)

- â **Extract team pass volume metrics**: Implemented in `te_team_pass_volume` and `te_team_pass_efficiency` features
- â **Add red zone target calculation**: Implemented in `te_rz_target_share` feature (most predictive for TEs)
- â **Fix output layer scaling**: Removed sigmoid compression, proper 2-40 point range implemented
- â **Add basic quantile outputs**: Multi-output architecture with ceiling/floor prediction capability
- â **Implement formation features**: Two-TE set usage and snap share analysis (`te_snap_share`, `te_two_te_sets`)
- â **Add opponent defense metrics**: TE defense strength and points allowed analysis (`te_opp_def_strength`)
- â **Create validation framework**: Comprehensive test script with architecture and feature validation
- â **Add Vegas features**: Implemented `te_vegas_total_correlation` and `te_vegas_implied_correlation` features
- â **Calculate opponent TE defense metrics**: Implemented in `te_opp_def_strength` with points allowed analysis
- â **Add TD regression features**: Implemented `te_td_rate` and TD prediction regularization in loss function
- â **Implement specialized training**: Custom `train_te_model()` method with variance protection and validation

### Advanced Features (If Time Permits)

- [ ] Route participation from play_by_play
- [ ] Slot vs inline alignment rates
- [ ] Pass block rates (negative for targets)
- [ ] Personnel grouping rates (12/13)
- [ ] Air yards and aDOT calculations

## Expected Improvements

### Current vs Target Performance

| Metric     | Current | Target  | Expected  |
| ---------- | ------- | ------- | --------- |
| RÂ²         | -0.886  | â¥0.20   | 0.25-0.35 |
| MAE        | 6.164   | 4.2-5.2 | 4.5-5.0   |
| Spearman   | ~0      | â¥0.28   | 0.30-0.40 |
| Pred Range | 0-4.9   | 0-35    | 2-30      |
| Pred Std   | <1      | >3      | 4-6       |

## Critical Success Factors

1. **Target share is king** - Must be feature #1 in importance
2. **TD access drives ceiling** - Red zone targets critical
3. **Role matters** - Pass blockers â  pass catchers
4. **Game script dependency** - Positive script = more TE usage
5. **QB connection** - TE-QB correlation should be >0.3

## Stacking & Correlation Rules

```python
def calculate_te_correlations(te_player, qb_player, game_total, spread):
    """Calculate DFS correlations for lineup building."""

    # TE-QB stack (primary)
    te_qb_corr = 0.35  # Base correlation
    if game_total > 48:
        te_qb_corr += 0.1  # Higher in shootouts

    # Bring-back correlation
    opp_wr1_corr = 0.15 if game_total > 50 and abs(spread) < 7 else 0.05

    # Within-team negative correlation
    wr1_corr = -0.1  # Slight negative with WR1
    if game_total < 42:
        wr1_corr = -0.2  # More negative in low-scoring games

    return {
        'qb': te_qb_corr,
        'opp_wr1': opp_wr1_corr,
        'team_wr1': wr1_corr
    }
```

## Testing Script

```python
# test_te_model.py
import numpy as np
from sklearn.metrics import mean_absolute_error, r2_score

def test_te_predictions():
    """Validate TE model fixes."""

    # Test known TEs
    test_players = [
        ('Travis Kelce', 'KC', 25),  # Elite TE, should project 12-18
        ('Mark Andrews', 'BAL', 20),  # High volume, 10-15
        ('George Kittle', 'SF', 15),  # YAC monster, 8-14
        ('Dawson Knox', 'BUF', 8),   # TD dependent, 4-10
        ('Tyler Higbee', 'LAR', 5)   # Floor play, 3-8
    ]

    for player, team, expected_avg in test_players:
        features = extract_te_features(player, team, 2024, 17, conn)
        X = np.array([list(features.values())])

        model = load_model('models/te_model.pth')
        outputs = model.predict(X)

        pred = outputs['mean'][0]
        floor = outputs['floor'][0]
        ceiling = outputs['ceiling'][0]

        print(f"{player:15} | Pred: {pred:.1f} | Floor: {floor:.1f} | "
              f"Ceiling: {ceiling:.1f} | Expected: ~{expected_avg}")

        # Validate reasonable ranges
        assert floor < pred < ceiling, f"Quantile ordering wrong for {player}"
        assert abs(pred - expected_avg) < 8, f"Way off for {player}"

    print("â TE model validation passed!")

if __name__ == "__main__":
    test_te_predictions()
```

## Deployment Steps

1. **Backup current model**: `cp models/te_model_nn.pth models/te_model_nn_backup.pth`
2. **Update TENetwork class** in models.py with multi-head architecture
3. **Add feature extraction** function to data.py
4. **Retrain with new pipeline**: `python run.py train --position TE`
5. **Validate predictions** meet targets
6. **Test stacking correlations** in optimizer

## Monitoring Post-Deployment

1. Track weekly MAE vs actual DFS scores
2. Monitor feature importance rankings
3. Check quantile calibration (coverage rates)
4. Validate stack correlations in winning lineups
5. Compare to industry projections for sanity

## Common Pitfalls to Avoid

1. **Not differentiating roles** - Travis Kelce â  blocking TE
2. **Ignoring pass block rate** - Kills target upside
3. **Missing team context** - Some teams don't use TEs
4. **Over-weighting TDs** - High variance, use RZ rate instead
5. **Weather overreaction** - Only matters for deep routes
6. **Not considering game script** - Trailing teams throw to TEs more

## Next Steps

1. Implement feature extraction function immediately
2. Fix neural network architecture (remove Sigmoid!)
3. Add quantile heads for floor/ceiling
4. Train with fixed epochs, checkpoint best RÂ²
5. Validate against 2024 actuals
6. Deploy to production
